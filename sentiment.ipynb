{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [00:00, 35961.45it/s]<?, ?it/s]\n",
      "150it [00:00, 51425.99it/s]\n",
      "150it [00:00, 51091.90it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 117.74it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "directory = 'Labelled Data'\n",
    " \n",
    "posts = []\n",
    "labels = []\n",
    "for filename in tqdm(os.listdir(directory)):\n",
    "    f = os.path.join(directory, filename)\n",
    "    if os.path.isfile(f):\n",
    "        parser_df = pd.read_csv(f)\n",
    "        for index, row in tqdm(parser_df.iterrows()):\n",
    "            posts.append(row['title_text_combined'])\n",
    "            labels.append(row['sentiment_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "61it [00:13,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Failed on post #59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [00:14,  9.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Failed on post #68\n",
      "Sentiment Analysis Failed on post #69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "186it [00:35,  9.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Failed on post #184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "209it [00:39,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Failed on post #207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "242it [00:45,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Failed on post #240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "322it [01:00,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Failed on post #320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "330it [01:02,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Failed on post #330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "450it [01:22,  5.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentiment_analysis import SentimentAnalyzer\n",
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "pred_labels = sentiment_analyzer.batch_analysis(posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charlie Accuracy: (0.5866666666666667, 150)\n",
      "Daniel Accuracy: (0.74, 150)\n",
      "Dillan Accuract: (0.7266666666666667, 150)\n",
      "Overall Accuracy (0.0 Confidence): (0.6844444444444444, 450)\n",
      "Overall Accuracy (.75 Confidence): (0.8356164383561644, 219)\n",
      "Neutral Count (Pred): 291\n",
      "Neutral Count (Labelled): 254\n"
     ]
    }
   ],
   "source": [
    "print (\"Charlie Accuracy:\", sentiment_analyzer.calculate_accuracy(pred_labels[:150], labels[:150]))\n",
    "print (\"Daniel Accuracy:\", sentiment_analyzer.calculate_accuracy(pred_labels[150:300], labels[150:300]))\n",
    "print (\"Dillan Accuract:\", sentiment_analyzer.calculate_accuracy(pred_labels[300:], labels[300:]))\n",
    "\n",
    "print (\"Overall Accuracy (0.0 Confidence):\", sentiment_analyzer.calculate_accuracy(pred_labels, labels))\n",
    "print (\"Overall Accuracy (.75 Confidence):\", sentiment_analyzer.calculate_accuracy(pred_labels, labels, 0.75))\n",
    "\n",
    "print (\"Neutral Count (Pred):\", len([i for i in pred_labels if i[0] == 'neutral']))\n",
    "print (\"Neutral Count (Labelled):\", len([i for i in labels if i == 0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0131f037897c358a9a268dfd2e11505808df673ecd6e283ac185a0b749caaaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
